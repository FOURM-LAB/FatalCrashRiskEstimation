{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48316e37",
   "metadata": {},
   "source": [
    "This script illustrate the Multi-Scale Cross-Matching pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3413df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mydataset import MyDataset_MultiScale_PreTrain\n",
    "import myaugmentation as MyAug\n",
    "import mymodels\n",
    "import myloss\n",
    "\n",
    "import os \n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.parallel as parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2048, hidden_size=512, output_size=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b91fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Get the names of the available GPUs\n",
    "    gpu_names = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
    "    print(\"Available GPUs:\")\n",
    "    for i, gpu_name in enumerate(gpu_names):\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6da0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "epochs = 25\n",
    "batch_size = 56\n",
    "num_workers = 2\n",
    "learning_rate = 1e-3\n",
    "optimizer_name = \"AdamW\"\n",
    "\n",
    "output_path = \".\"\n",
    "model_name = \"ResNet50_Pre-Train\"\n",
    "metadataTrain = \"./metadata/train.csv\" # list of training files\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter(log_dir=os.path.join(output_path, model_name, \"log\", \"pre-training\"))\n",
    "\n",
    "# Create Dataset and Dataloader\n",
    "dataset = MyDataset_MultiScale_PreTrain(basic_transform=MyAug.base_aug(), \n",
    "                                        aug_transform=MyAug.pretrain_aug(),\n",
    "                                        metadata=metadataTrain)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                        num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197240d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and prepare for training\n",
    "model = mymodels.ResNet50_with_Feature()\n",
    "model = parallel.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion_infonce = myloss.InfoNCE()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = GradScaler() # the scaler for mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dabe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # for accuracy\n",
    "    total = 0\n",
    "    correct = 0 # w/o augmentation\n",
    "        \n",
    "    for i, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "        # Move the data to the device\n",
    "        image_1 = batch[1].to(device) # the image patch\n",
    "        image_2 = batch[2].to(device) # the whole image w/ mask\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            _, image_embeddings = model(image_1)\n",
    "            _, patch_embeddings = model(image_2)\n",
    "            \n",
    "            # InfoNCE Loss\n",
    "            loss = criterion_infonce(image_embeddings, patch_embeddings                                                \n",
    "\n",
    "        # Perform the backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        # Log training loss to TensorBoard\n",
    "        writer.add_scalar('Running/Loss', loss.item(), epoch*len(dataloader)+i)        \n",
    "        writer.add_scalar('Running/Loss-InfoNCE', loss_infonce.item(), epoch*len(dataloader)+i)\n",
    "        writer.add_scalar('Running/LR', optimizer.param_groups[0]['lr'], epoch*len(dataloader)+i)   \n",
    "        \n",
    "        \n",
    "    # Calculate the average loss\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    writer.add_scalar('Epoch/Loss', epoch_loss, epoch)\n",
    "\n",
    "    if use_classifier:\n",
    "        accuracy_overall = correct / total\n",
    "        writer.add_scalar('Epoch/Acc', accuracy_overall, epoch)\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(output_path, model_name+f'_{epoch}_{epoch_loss}.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
